# openai_backend.py â€” Drive refresh + Ã­ndice sincronizado + caching por arquivo

import os
import io
import re
import json
import time
import unicodedata
import numpy as np
import requests
import streamlit as st # LINHA DE IMPORT DO STREAMLIT
from html import escape
from docx import Document
from googleapiclient.discovery import build
from google.oauth2 import service_account

# ========= CONFIG BÃSICA (ATRASANDO A LEITURA DE SECRETS) =========

def get_api_key():
    """LÃª a chave de API do st.secrets, atrasando a chamada."""
    try:
        # Acessa st.secrets DENTRO da funÃ§Ã£o, que sÃ³ serÃ¡ chamada DEPOIS do app.py
        return st.secrets["openai"]["api_key"].strip()
    except Exception:
        # Fallback ou erro, se nÃ£o conseguir ler
        # Use a variÃ¡vel de ambiente se estiver disponÃ­vel, senÃ£o uma chave dummy
        return os.environ.get("OPENAI_API_KEY", "DUMMY_KEY_IF_NOT_FOUND")

# O MODEL_ID pode ser definido como global
MODEL_ID = "gpt-4o-mini"

# ========= PERFORMANCE & QUALIDADE (MODO TURBO) =========
USE_JSONL = True
USE_CE = False
SKIP_CE_IF_ANN_BEST = 0.80

# menos candidatos na ANN e mais blocos no contexto
TOP_N_ANN = 12
TOP_K = 5 # â¬…ï¸ MODIFICAÃ‡ÃƒO 1: Mais contexto (de 3 para 5)

# blocos menores e janela menor (contexto mais enxuto)
MAX_WORDS_PER_BLOCK = 160
GROUP_WINDOW = 2

CE_SCORE_THRESHOLD = 0.38
ANN_SCORE_THRESHOLD = 0.15 # â¬…ï¸ MODIFICAÃ‡ÃƒO 2: Limite de score mais baixo (de 0.18 para 0.15)

# resposta mais curta
MAX_TOKENS = 280

REQUEST_TIMEOUT = 20
TEMPERATURE = 0.15

# ========= ÃNDICE PRÃ‰-COMPUTADO (opcional) =========
PRECOMP_FAISS_NAME = "faiss.index"
PRECOMP_VECTORS_NAME = "vectors.npy"
PRECOMP_BLOCKS_NAME = "blocks.json"
USE_PRECOMPUTED = False

# ========= DRIVE / AUTH =========
FOLDER_ID = "1fdcVl6RcoyaCpa6PmOX1kUAhXn5YIPTa"
SCOPES = ['https://www.googleapis.com/auth/drive.readonly']

# ========= FALLBACK =========
FALLBACK_MSG = (
Â  Â  "âš ï¸ Este agente Ã© exclusivo para consulta de Procedimento Operacional PadrÃ£o - POP Quadra. âš ï¸\n"
Â  Â  "Departamento de EstratÃ©gia & InovaÃ§Ã£o."
)

# ========= CACHE BUSTER =========
# mudei de novo pra forÃ§ar rebuild de tudo
CACHE_BUSTER = "2025-11-19-LINK-FIX-02"

# ========= HTTP SESSION (CHAVE LIDA VIA FUNÃ‡ÃƒO) =========
def build_session_headers():
    key = get_api_key()
    return {
        "Authorization": f"Bearer {key}",
        "Content-Type": "application/json"
    }

session = requests.Session()
session.headers.update(build_session_headers())


# ========================= UTILS =========================
def sanitize_doc_name(name: str) -> str:
Â  Â  name = re.sub(r"^(C[oÃ³]pia de|Copy of)\s+", "", name, flags=re.IGNORECASE)
Â  Â  name = re.sub(r"\.(docx?|pdf|txt|jsonl?|JSONL?)$", "", name, flags=re.IGNORECASE)
Â  Â  return name.strip()


def _strip_accents(s: str) -> str:
Â  Â  return "".join(c for c in unicodedata.normalize("NFD", s) if unicodedata.category(c) != "Mn")


def _tokenize(s: str):
Â  Â  s = _strip_accents((s or "").lower())
Â  Â  return re.findall(r"[a-zA-Z0-9_]{3,}", s)


def _has_lexical_evidence(query: str, texts: list[str]) -> bool:
Â  Â  q_tokens = set(_tokenize(query))
Â  Â  if not q_tokens:
Â  Â  Â  Â  return False
Â  Â  for t in texts:
Â  Â  Â  Â  t_tokens = set(_tokenize(t or ""))
Â  Â  Â  Â  if q_tokens & t_tokens:
Â  Â  Â  Â  Â  Â  return True
Â  Â  return False


def _is_fallback_output(text: str) -> bool:
Â  Â  if not text:
Â  Â  Â  Â  return False
Â  Â  norm = "\n".join([line.strip() for line in text.strip().splitlines() if line.strip()])
Â  Â  norm = _strip_accents(norm.lower())
Â  Â  fallback_norm = _strip_accents(FALLBACK_MSG.lower())
Â  Â  first_line = _strip_accents(FALLBACK_MSG.splitlines()[0].lower())
Â  Â  return (fallback_norm in norm) or norm.startswith(first_line)


_NOINFO_RE = re.compile(
Â  Â  r"(nÃ£o\s+hÃ¡\s+informa|nÃ£o\s+encontrei|nÃ£o\s+foi\s+possÃ­vel\s+encontrar|sem\s+informaÃ§Ãµes|nÃ£o\s+consta|nÃ£o\s+existe)",
Â  Â  re.IGNORECASE
)


def _looks_like_noinfo(text: str) -> bool:
Â  Â  return bool(text and _NOINFO_RE.search(text))


def _expand_query_for_hr(query: str) -> str:
Â  Â  """
Â  Â  Expande algumas queries curtas de RH para melhorar o match semÃ¢ntico.
Â  Â  Ex: 'contratar', 'contrataÃ§Ã£o', 'admissÃ£o' etc.
Â  Â  """
Â  Â  q_norm = _strip_accents(query.lower())
Â  Â  extras = []

Â  Â  if "contrat" in q_norm:
Â  Â  Â  Â  extras.append(
Â  Â  Â  Â  Â  Â  "contrataÃ§Ã£o de funcionÃ¡rios admissÃ£o de colaboradores "
Â  Â  Â  Â  Â  Â  "processo de admissÃ£o contrataÃ§Ã£o de pessoal recrutamento seleÃ§Ã£o de candidatos"
Â  Â  Â  Â  )

Â  Â  if "admiss" in q_norm:
Â  Â  Â  Â  extras.append(
Â  Â  Â  Â  Â  Â  "admissÃ£o de pessoal contrataÃ§Ã£o de funcionÃ¡rios contrataÃ§Ã£o de colaboradores "
Â  Â  Â  Â  Â  Â  "processo de admissÃ£o recrutamento"
Â  Â  Â  Â  )

Â  Â  if extras:
Â  Â  Â  Â  return query + " " + " ".join(extras)
Â  Â  return query


# --------- auxÃ­lio para escolher o documento certo pro link ----------
def _overlap_score(a: str, b: str) -> float:
Â  Â  """
Â  Â  Score simples de sobreposiÃ§Ã£o lexical entre dois textos.
Â  Â  """
Â  Â  ta = set(_tokenize(a))
Â  Â  tb = set(_tokenize(b))
Â  Â  if not ta or not tb:
Â  Â  Â  Â  return 0.0
Â  Â  inter = len(ta & tb)
Â  Â  # peso pelo tamanho da resposta para nÃ£o favorecer textos gigantes
Â  Â  return inter / max(1.0, len(ta))


def _escolher_bloco_para_link(pergunta: str, resposta: str, blocos: list[dict]):
Â  Â  """
Â  Â  Escolhe o bloco cujo texto mais se parece com a RESPOSTA (e, de quebra, com a pergunta).
Â  Â  Assim, se a resposta falar de Marketing, tende a escolher o bloco do POP de Marketing,
Â  Â  mesmo que o primeiro bloco do contexto seja de Compras.
Â  Â  """
Â  Â  if not blocos:
Â  Â  Â  Â  return None

Â  Â  melhor = None
Â  Â  melhor_score = 0.0
Â  Â  for b in blocos:
Â  Â  Â  Â  txt = b.get("texto") or ""
Â  Â  Â  Â  if not txt.strip():
Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  s_resp = _overlap_score(resposta or "", txt)
Â  Â  Â  Â  s_perg = _overlap_score(pergunta or "", txt)
Â  Â  Â  Â  score = s_resp + 0.5 * s_pergÂ  # resposta pesa mais que a pergunta
Â  Â  Â  Â  if score > melhor_score:
Â  Â  Â  Â  Â  Â  melhor_score = score
Â  Â  Â  Â  Â  Â  melhor = b

Â  Â  # se o score for muito baixo, melhor nÃ£o linkar nada pra nÃ£o errar
Â  Â  if melhor is None or melhor_score < 0.02:
Â  Â  Â  Â  return None
Â  Â  return melhor


# ========================= FALLBACK INTERATIVO =========================
def gerar_resposta_fallback_interativa(pergunta: str,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â api_key: str = None, # NÃ£o usamos mais API_KEY global
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â model_id: str = MODEL_ID) -> str:
Â  Â  """
Â  Â  Gera uma resposta mais conversada quando nÃ£o hÃ¡ informaÃ§Ã£o nos POPs.
Â  Â  """
Â  Â  try:
Â  Â  Â  Â  prompt_usuario = (
Â  Â  Â  Â  Â  Â  "O usuÃ¡rio fez a pergunta abaixo, mas nÃ£o encontramos nenhum conteÃºdo correspondente "
Â  Â  Â  Â  Â  Â  "nos documentos internos ou POPs da Quadra Engenharia.\n\n"
Â  Â  Â  Â  Â  Â  "Sua tarefa:\n"
Â  Â  Â  Â  Â  Â  "1. Cumprimente o usuÃ¡rio de forma cordial.\n"
Â  Â  Â  Â  Â  Â  "2. Explique que vocÃª Ã© um assistente treinado principalmente com documentos internos "
Â  Â  Â  Â  Â  Â  "Â  Â (procedimentos, POPs, rotinas, fluxos da Quadra) e que nÃ£o localizou nada especÃ­fico "
Â  Â  Â  Â  Â  Â  "Â  Â sobre essa pergunta nos documentos.\n"
Â  Â  Â  Â  Â  Â  "3. Se a pergunta for claramente de conhecimento geral (por exemplo, eventos pÃºblicos, "
Â  Â  Â  Â  Â  Â  "Â  Â datas comemorativas, conceitos amplos), vocÃª pode dar uma resposta curta com base "
Â  Â  Â  Â  Â  Â  "Â  Â em conhecimento geral, deixando claro que isso vem de informaÃ§Ãµes pÃºblicas e nÃ£o "
Â  Â  Â  Â  Â  Â  "Â  Â de documentos da Quadra.\n"
Â  Â  Â  Â  Â  Â  "4. Ajude o usuÃ¡rio a continuar: sugira que ele reformule a dÃºvida focando em processos, "
Â  Â  Â  Â  Â  Â  "Â  Â polÃ­ticas, POPs, rotinas ou documentos da Quadra, **com uma pergunta de fechamento amigÃ¡vel (Ex: 'VocÃª gostaria de tentar reformular a pergunta com foco em um processo interno?')**.\n" # â¬…ï¸ MODIFICAÃ‡ÃƒO 4: SugestÃ£o amigÃ¡vel
Â  Â  Â  Â  Â  Â  "5. Use um tom profissional, mas prÃ³ximo e amigÃ¡vel, como em uma conversa. "
Â  Â  Â  Â  Â  Â  "Â  Â Evite listas muito longas (no mÃ¡ximo 3 itens) e responda em portuguÃªs do Brasil.\n\n"
Â  Â  Â  Â  Â  Â  f"Pergunta do usuÃ¡rio:\n\"{pergunta}\""
Â  Â  Â  Â  )

Â  Â  Â  Â  payload = {
Â  Â  Â  Â  Â  Â  "model": model_id,
Â  Â  Â  Â  Â  Â  "messages": [
Â  Â  Â  Â  Â  Â  Â  Â  {
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "role": "system",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "content": (
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "VocÃª Ã© um assistente virtual da Quadra Engenharia. "
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Responda sempre em portuguÃªs do Brasil, de forma clara, educada e objetiva."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ),
Â  Â  Â  Â  Â  Â  Â  Â  },
Â  Â  Â  Â  Â  Â  Â  Â  {"role": "user", "content": prompt_usuario},
Â  Â  Â  Â  Â  Â  ],
Â  Â  Â  Â  Â  Â  "max_tokens": max(320, MAX_TOKENS),
Â  Â  Â  Â  Â  Â  "temperature": 0.35,
Â  Â  Â  Â  Â  Â  "n": 1,
Â  Â  Â  Â  Â  Â  "stream": False,
Â  Â  Â  Â  }

Â  Â  Â  Â  resp = session.post(
Â  Â  Â  Â  Â  Â  "https://api.openai.com/v1/chat/completions",
Â  Â  Â  Â  Â  Â  json=payload,
Â  Â  Â  Â  Â  Â  timeout=REQUEST_TIMEOUT,
Â  Â  Â  Â  )
Â  Â  Â  Â  resp.raise_for_status()
Â  Â  Â  Â  data = resp.json()
Â  Â  Â  Â  texto = (
Â  Â  Â  Â  Â  Â  data.get("choices", [{}])[0]
Â  Â  Â  Â  Â  Â  Â  Â  .get("message", {})
Â  Â  Â  Â  Â  Â  Â  Â  .get("content", "")
Â  Â  Â  Â  )
Â  Â  Â  Â  if not texto or not texto.strip():
Â  Â  Â  Â  Â  Â  return FALLBACK_MSG
Â  Â  Â  Â  return texto.strip()

Â  Â  except requests.exceptions.RequestException as e:
Â  Â  Â  Â  print(f"[DEBUG POP-BOT] Erro na chamada de fallback interativo: {e}")
Â  Â  Â  Â  return FALLBACK_MSG
Â  Â  except Exception as e:
Â  Â  Â  Â  print(f"[DEBUG POP-BOT] Erro inesperado no fallback interativo: {e}")
Â  Â  Â  Â  return FALLBACK_MSG


# ========================= CLIENTES CACHEADOS =========================
@st.cache_resource(show_spinner=False)
def get_drive_client(_v=CACHE_BUSTER):
Â  Â  creds = service_account.Credentials.from_service_account_info(
Â  Â  Â  Â  dict(st.secrets["gcp_service_account"]), scopes=SCOPES
Â  Â  )
Â  Â  return build('drive', 'v3', credentials=creds)


@st.cache_resource(show_spinner=False)
def get_sbert_model(_v=CACHE_BUSTER):
Â  Â  from sentence_transformers import SentenceTransformer
Â  Â  return SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")


@st.cache_resource(show_spinner=False)
def get_cross_encoder(_v=CACHE_BUSTER):
Â  Â  if not USE_CE:
Â  Â  Â  Â  return None
Â  Â  import torch
Â  Â  from sentence_transformers import CrossEncoder
Â  Â  device = "cuda" if torch.cuda.is_available() else "cpu"
Â  Â  return CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2", device=device)


# ========================= DRIVE LIST/DOWNLOAD =========================
def _drive_list_all(drive_service, query: str, fields: str):
Â  Â  all_files = []
Â  Â  page_token = None
Â  Â  while True:
Â  Â  Â  Â  resp = drive_service.files().list(
Â  Â  Â  Â  Â  Â  q=query,
Â  Â  Â  Â  Â  Â  fields=f"nextPageToken,{fields}",
Â  Â  Â  Â  Â  Â  pageSize=1000,
Â  Â  Â  Â  Â  Â  pageToken=page_token,
Â  Â  Â  Â  Â  Â  includeItemsFromAllDrives=True,
Â  Â  Â  Â  Â  Â  supportsAllDrives=True,
Â  Â  Â  Â  Â  Â  corpora="allDrives"
Â  Â  Â  Â  ).execute()
Â  Â  Â  Â  items = resp.get("files", []) or []
Â  Â  Â  Â  all_files.extend(items)
Â  Â  Â  Â  page_token = resp.get("nextPageToken")
Â  Â  Â  Â  if not page_token:
Â  Â  Â  Â  Â  Â  break
Â  Â  return all_files


def _list_by_mime_query(drive_service, folder_id, mime_query):
Â  Â  query = f"'{folder_id}' in parents and ({mime_query}) and trashed = false"
Â  Â  fields = "files(id, name, md5Checksum, modifiedTime, mimeType)"
Â  Â  return _drive_list_all(drive_service, query, fields)


def _list_docx_metadata(drive_service, folder_id):
Â  Â  return _list_by_mime_query(
Â  Â  Â  Â  drive_service, folder_id,
Â  Â  Â  Â  "mimeType='application/vnd.openxmlformats-officedocument.wordprocessingml.document'"
Â  Â  )


def _list_json_metadata(drive_service, folder_id):
Â  Â  files = _list_by_mime_query(
Â  Â  Â  Â  drive_service, folder_id,
Â  Â  Â  Â  "mimeType='application/json' or mimeType='text/plain'"
Â  Â  )
Â  Â  return [f for f in files if f.get("name", "").lower().endswith((".jsonl", ".json"))]


def _list_named_files(drive_service, folder_id, wanted_names):
Â  Â  fields = "files(id, name, md5Checksum, modifiedTime, mimeType)"
Â  Â  query = f"'{folder_id}' in parents and trashed = false"
Â  Â  files = _drive_list_all(drive_service, query, fields)
Â  Â  return {f["name"]: f for f in files if f.get("name") in wanted_names}


def _download_bytes(drive_service, file_id):
Â  Â  request = drive_service.files().get_media(fileId=file_id, supportsAllDrives=True)
Â  Â  return request.execute()


def _download_text(drive_service, file_id) -> str:
Â  Â  return _download_bytes(drive_service, file_id).decode("utf-8", errors="ignore")


# ========================= PARSE DOCX/JSON =========================
def _split_text_blocks(text, max_words=MAX_WORDS_PER_BLOCK):
Â  Â  words = text.split()
Â  Â  return [" ".join(words[i:i + max_words]) for i in range(0, len(words), max_words)]


def _docx_to_blocks(file_bytes, file_name, file_id, max_words=MAX_WORDS_PER_BLOCK):
Â  Â  doc = Document(io.BytesIO(file_bytes))
Â  Â  text = "\n".join([p.text.strip() for p in doc.paragraphs if p.text.strip()])
Â  Â  return [
Â  Â  Â  Â  {"pagina": file_name, "texto": chunk, "file_id": file_id}
Â  Â  Â  Â  for chunk in _split_text_blocks(text, max_words=max_words) if chunk.strip()
Â  Â  ]


def _records_from_json_text(text: str):
Â  Â  recs = []
Â  Â  t = text.lstrip()
Â  Â  if t.startswith("["):
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  data = json.loads(t)
Â  Â  Â  Â  Â  Â  if isinstance(data, list):
Â  Â  Â  Â  Â  Â  Â  Â  recs = data
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  pass
Â  Â  else:
Â  Â  Â  Â  for line in text.splitlines():
Â  Â  Â  Â  Â  Â  line = line.strip()
Â  Â  Â  Â  Â  Â  if not line:
Â  Â  Â  Â  Â  Â  Â  Â  continue
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  recs.append(json.loads(line))
Â  Â  Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  Â  Â  continue
Â  Â  return recs


def _json_records_to_blocks(recs, fallback_name: str, file_id: str):
Â  Â  out = []
Â  Â  for r in recs:
Â  Â  Â  Â  pagina = r.get("pagina") or r.get("page") or r.get("doc") or fallback_name
Â  Â  Â  Â  texto = r.get("texto") or r.get("text") or r.get("content") or ""
Â  Â  Â  Â  fid = r.get("file_id") or r.get("source_id") or file_id
Â  Â  Â  Â  if str(texto).strip():
Â  Â  Â  Â  Â  Â  out.append({"pagina": str(pagina), "texto": str(texto), "file_id": fid})
Â  Â  return out


# ========================= CACHE DE FONTE (DOCX/JSON) =========================
@st.cache_data(show_spinner=False, ttl=600)
def _list_sources_cached(folder_id: str, _v=CACHE_BUSTER):
Â  Â  drive = get_drive_client()
Â  Â  files_json = _list_json_metadata(drive, folder_id) if USE_JSONL else []
Â  Â  files_docx = _list_docx_metadata(drive, folder_id)
Â  Â  return {"json": files_json, "docx": files_docx}


def _signature_from_files(files):
Â  Â  return [{k: f.get(k) for k in ("id", "name", "md5Checksum", "modifiedTime")} for f in (files or [])]


def _build_signature_json_docx(files_json, files_docx):
Â  Â  payload = {
Â  Â  Â  Â  "json": sorted(_signature_from_files(files_json), key=lambda x: x["id"]) if files_json else [],
Â  Â  Â  Â  "docx": sorted(_signature_from_files(files_docx), key=lambda x: x["id"]) if files_docx else [],
Â  Â  }
Â  Â  return json.dumps(payload, ensure_ascii=False)


@st.cache_data(show_spinner=False)
def _parse_docx_cached(file_id: str, md5: str, name: str):
Â  Â  drive = get_drive_client()
Â  Â  return _docx_to_blocks(_download_bytes(drive, file_id), name, file_id)


@st.cache_data(show_spinner=False)
def _parse_json_cached(file_id: str, md5: str, name: str):
Â  Â  drive = get_drive_client()
Â  Â  recs = _records_from_json_text(_download_text(drive, file_id))
Â  Â  return _json_records_to_blocks(recs, fallback_name=name, file_id=file_id)


@st.cache_data(show_spinner=False)
def _download_and_parse_blocks(signature: str, folder_id: str, _v=CACHE_BUSTER):
Â  Â  sources = _list_sources_cached(folder_id)
Â  Â  files_json = sources.get("json", []) if USE_JSONL else []
Â  Â  files_docx = sources.get("docx", []) or []

Â  Â  blocks = []

Â  Â  for f in files_json:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  md5 = f.get("md5Checksum", f.get("modifiedTime", ""))
Â  Â  Â  Â  Â  Â  blocks.extend(_parse_json_cached(f["id"], md5, f["name"]))
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  continue

Â  Â  for f in files_docx:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  md5 = f.get("md5Checksum", f.get("modifiedTime", ""))
Â  Â  Â  Â  Â  Â  blocks.extend(_parse_docx_cached(f["id"], md5, f["name"]))
Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  continue

Â  Â  return blocks


def load_all_blocks_cached(folder_id: str):
Â  Â  src = _list_sources_cached(folder_id)
Â  Â  signature = _build_signature_json_docx(src.get("json", []), src.get("docx", []))
Â  Â  blocks = _download_and_parse_blocks(signature, folder_id)
Â  Â  return blocks, signature


# ========================= AGRUPAMENTO =========================
def agrupar_blocos(blocos, janela=GROUP_WINDOW):
Â  Â  """
Â  Â  Agrupa blocos em janelas, mas **sem misturar documentos diferentes**.
Â  Â  Se o prÃ³ximo bloco tiver file_id diferente, o grupo Ã© cortado ali.
Â  Â  """
Â  Â  grouped = []
Â  Â  n = len(blocos)
Â  Â  if n == 0:
Â  Â  Â  Â  return grouped

Â  Â  for i in range(n):
Â  Â  Â  Â  base = blocos[i]
Â  Â  Â  Â  current_file_id = base.get("file_id")
Â  Â  Â  Â  group = [base]

Â  Â  Â  Â  for offset in range(1, janela):
Â  Â  Â  Â  Â  Â  j = i + offset
Â  Â  Â  Â  Â  Â  if j >= n:
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  Â  Â  b_next = blocos[j]
Â  Â  Â  Â  Â  Â  if b_next.get("file_id") != current_file_id:
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  Â  Â  group.append(b_next)

Â  Â  Â  Â  grouped.append({
Â  Â  Â  Â  Â  Â  "pagina": group[0].get("pagina", "?"),
Â  Â  Â  Â  Â  Â  "texto": " ".join(b["texto"] for b in group),
Â  Â  Â  Â  Â  Â  "file_id": current_file_id,
Â  Â  Â  Â  })

Â  Â  return grouped


# ========================= ÃNDICE PRÃ‰-COMPUTADO =========================
def _list_named_files_map():
Â  Â  drive = get_drive_client()
Â  Â  want = {PRECOMP_FAISS_NAME, PRECOMP_VECTORS_NAME, PRECOMP_BLOCKS_NAME}
Â  Â  name_map = _list_named_files(drive, FOLDER_ID, want)
Â  Â  return name_map if all(n in name_map for n in want) else None


@st.cache_resource(show_spinner=False)
def _load_precomputed_index(_v=CACHE_BUSTER):
Â  Â  if not USE_PRECOMPUTED:
Â  Â  Â  Â  return None
Â  Â  ids_map = _list_named_files_map()
Â  Â  if not ids_map:
Â  Â  Â  Â  return None
Â  Â  drive = get_drive_client()
Â  Â  try:
Â  Â  Â  Â  vectors = np.load(io.BytesIO(_download_bytes(drive, ids_map[PRECOMP_VECTORS_NAME]["id"])))
Â  Â  Â  Â  blocks_json = json.loads(_download_text(drive, ids_map[PRECOMP_BLOCKS_NAME]["id"]))
Â  Â  Â  Â  blocks = _json_records_to_blocks(blocks_json, fallback_name="precomp", file_id="precomp")
Â  Â  Â  Â  import faiss
Â  Â  Â  Â  faiss_index_bytes = _download_bytes(drive, ids_map[PRECOMP_FAISS_NAME]["id"])
Â  Â  Â  Â  tmp_path = "/tmp/faiss.index"
Â  Â  Â  Â  with open(tmp_path, "wb") as f:
Â  Â  Â  Â  Â  Â  f.write(faiss_index_bytes)
Â  Â  Â  Â  index = faiss.read_index(tmp_path)
Â  Â  Â  Â  return {"blocks": blocks, "emb": vectors, "index": index, "use_faiss": True}
Â  Â  except Exception:
Â  Â  Â  Â  return None


# ========================= ÃNDICE (GERAR OU USAR PRONTO) =========================
def try_import_faiss():
Â  Â  try:
Â  Â  Â  Â  import faiss
Â  Â  Â  Â  return faiss
Â  Â  except Exception:
Â  Â  Â  Â  return None


@st.cache_resource(show_spinner=False)
def build_vector_index(signature: str, _v=CACHE_BUSTER):
Â  Â  pre = _load_precomputed_index()
Â  Â  if pre is not None:
Â  Â  Â  Â  return pre

Â  Â  blocks_raw, _sig = load_all_blocks_cached(FOLDER_ID)
Â  Â  grouped = agrupar_blocos(blocks_raw, janela=GROUP_WINDOW)
Â  Â  if not grouped:
Â  Â  Â  Â  return {"blocks": [], "emb": None, "index": None, "use_faiss": False}

Â  Â  sbert = get_sbert_model()
Â  Â  texts = [b["texto"] for b in grouped]

Â  Â  @st.cache_data(show_spinner=False)
Â  Â  def _embed_texts_cached(texts_, sig: str, _v2=CACHE_BUSTER):
Â  Â  Â  Â  return sbert.encode(texts_, convert_to_numpy=True, normalize_embeddings=True)

Â  Â  emb = _embed_texts_cached(texts, signature)

Â  Â  faiss = try_import_faiss()
Â  Â  use_faiss = False
Â  Â  index = None
Â  Â  if faiss is not None:
Â  Â  Â  Â  dim = emb.shape[1]
Â  Â  Â  Â  index = faiss.IndexFlatIP(dim)
Â  Â  Â  Â  index.add(emb.astype(np.float32))
Â  Â  Â  Â  use_faiss = True

Â  Â  return {"blocks": grouped, "emb": emb, "index": index, "use_faiss": use_faiss}


def get_vector_index():
Â  Â  _blocks, signature = load_all_blocks_cached(FOLDER_ID)
Â  Â  return build_vector_index(signature)


# ========================= BUSCA ANN =========================
def ann_search(query_text: str, top_n: int):
Â  Â  vecdb = get_vector_index()
Â  Â  blocks = vecdb["blocks"]
Â  Â  if not blocks:
Â  Â  Â  Â  return []

Â  Â  sbert = get_sbert_model()

Â  Â  query_for_embed = _expand_query_for_hr(query_text)

Â  Â  q = sbert.encode([query_for_embed], convert_to_numpy=True, normalize_embeddings=True)[0]

Â  Â  if vecdb["use_faiss"]:
Â  Â  Â  Â  D, I = vecdb["index"].search(q.reshape(1, -1).astype(np.float32), top_n)
Â  Â  Â  Â  idxs = I[0].tolist()
Â  Â  Â  Â  scores = D[0].tolist()
Â  Â  else:
Â  Â  Â  Â  emb = vecdb["emb"]
Â  Â  Â  Â  scores_all = (emb @ q)
Â  Â  Â  Â  idxs = np.argsort(-scores_all)[:top_n].tolist()
Â  Â  Â  Â  scores = [float(scores_all[i]) for i in idxs]

Â  Â  return [{"idx": i, "score": float(s), "block": blocks[i]} for i, s in zip(idxs, scores) if i >= 0]


# ========================= RERANKING (CE OPCIONAL) =========================
def crossencoder_rerank(query: str, candidates, top_k: int):
Â  Â  if not candidates:
Â  Â  Â  Â  return []
Â  Â  ce = get_cross_encoder()
Â  Â  if ce is None:
Â  Â  Â  Â  packed = [{"block": c["block"], "score": float(c["score"])} for c in candidates]
Â  Â  Â  Â  packed.sort(key=lambda x: x["score"], reverse=True)
Â  Â  Â  Â  return packed[:top_k]

Â  Â  pairs = [(query, c["block"]["texto"]) for c in candidates]
Â  Â  scores = ce.predict(pairs, batch_size=96)
Â  Â  packed = [{"block": c["block"], "score": float(s)} for c, s in zip(candidates, scores)]
Â  Â  packed.sort(key=lambda x: x["score"], reverse=True)
Â  Â  return packed[:top_k]


# ========================= PROMPT (MODO ENXUTO) =========================
def montar_prompt_rag(pergunta, blocos):
Â  Â  if not blocos:
Â  Â  Â  Â  return (
Â  Â  Â  Â  Â  Â  "VocÃª Ã© um assistente da Quadra especializado em orientar colaboradores sobre PROCEDIMENTOS INTERNOS.\n"
Â  Â  Â  Â  Â  Â  "Responda em prosa, de forma breve e direta.\n"
Â  Â  Â  Â  Â  Â  "SÃ³ responda se a pergunta estiver claramente relacionada a procedimentos internos corporativos "
Â  Â  Â  Â  Â  Â  "(RH, fÃ©rias, reembolso, compras, suprimentos, financeiro, TI, acesso, seguranÃ§a do trabalho, obras, qualidade, jurÃ­dico).\n"
Â  Â  Â  Â  Â  Â  f"Se nÃ£o estiver relacionado, responda exatamente o texto abaixo, sem acrescentar nada:\n"
Â  Â  Â  Â  Â  Â  f"{FALLBACK_MSG}\n\n"
Â  Â  Â  Â  Â  Â  f"Pergunta: {pergunta}\n\n"
Â  Â  Â  Â  Â  Â  "â¡ï¸ Resposta:"
Â  Â  Â  Â  )

Â  Â  contexto_parts = []
Â  Â  for b in blocos:
Â  Â  Â  Â  texto = b["texto"] or ""
Â  Â  Â  Â  if len(texto) > 1200:
Â  Â  Â  Â  Â  Â  texto = texto[:1200]
Â  Â  Â  Â  contexto_parts.append(f"[Documento {b.get('pagina', '?')}]:\n{texto}")

Â  Â  contexto_str = "\n\n".join(contexto_parts)

Â  Â  return (
Â  Â  Â  Â  "VocÃª Ã© o assistente virtual da Quadra Engenharia, especializado em Procedimentos Operacionais (POPs).\n"
Â  Â  Â  Â  "**Sua principal tarefa Ã© ser prestativo, amigÃ¡vel e usar uma linguagem natural, como em uma conversa.**\n" # â¬…ï¸ MODIFICAÃ‡ÃƒO 3: Tom Conversacional
Â  Â  Â  Â  "1. Responda Ã  pergunta do usuÃ¡rio **apenas** com base nas informaÃ§Ãµes fornecidas nos blocos de contexto.\n"
Â  Â  Â  Â  "**2. Utilize formataÃ§Ã£o Markdown (parÃ¡grafos, listas, negrito) para tornar a resposta clara, organizada e fÃ¡cil de ler.**\n" # â¬…ï¸ MODIFICAÃ‡ÃƒO 3: Uso de Markdown
Â  Â  Â  Â  "3. Mantenha um tom profissional, mas acessÃ­vel e cordial.\n"
Â  Â  Â  Â  "4. Se as informaÃ§Ãµes forem insuficientes ou nÃ£o relacionadas, vocÃª deve explicar de forma educada que a informaÃ§Ã£o nÃ£o consta nos POPs internos, **sugerindo que o usuÃ¡rio reformule a pergunta focando em processos ou rotinas da Quadra.**\n" # â¬…ï¸ MODIFICAÃ‡ÃƒO 3: Fallback suave
Â  Â  Â  Â  f"Contexto com os POPs da Quadra:\n{contexto_str}\n\n"
Â  Â  Â  Â  f"Pergunta do usuÃ¡rio: {pergunta}\n\n"
Â  Â  Â  Â  "â¡ï¸ Resposta detalhada (em prosa e/ou lista com Markdown):" # â¬…ï¸ MODIFICAÃ‡ÃƒO 3: InstruÃ§Ã£o de resposta detalhada
Â  Â  )


# ========================= PRINCIPAL =========================
def responder_pergunta(pergunta, top_k: int = TOP_K, model_id: str = MODEL_ID): # Removi api_key dos argumentos
Â  Â  t0 = time.perf_counter()
Â  Â  try:
Â  Â  Â  Â  pergunta = (pergunta or "").strip().replace("\n", " ").replace("\r", " ")
Â  Â  Â  Â  if not pergunta:
Â  Â  Â  Â  Â  Â  return "âš ï¸ Pergunta vazia."

Â  Â  Â  Â  # 1) Busca ANN
Â  Â  Â  Â  candidates = ann_search(pergunta, top_n=TOP_N_ANN)
Â  Â  Â  Â  if not candidates:
Â  Â  Â  Â  Â  Â  # Agora a chave Ã© lida internamente na funÃ§Ã£o
Â  Â  Â  Â  Â  Â  return gerar_resposta_fallback_interativa(pergunta, model_id=model_id)

Â  Â  Â  Â  candidates.sort(key=lambda x: x["score"], reverse=True)
Â  Â  Â  Â  best_ann = candidates[0]["score"]

Â  Â  Â  Â  # 2) Decide se usa CrossEncoder (desativado no momento)
Â  Â  Â  Â  run_ce = USE_CE and (best_ann < SKIP_CE_IF_ANN_BEST)
Â  Â  Â  Â  if run_ce:
Â  Â  Â  Â  Â  Â  subset = candidates[:12]
Â  Â  Â  Â  Â  Â  reranked = crossencoder_rerank(pergunta, subset, top_k=top_k)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  reranked = [{"block": c["block"], "score": c["score"]} for c in candidates[:top_k]]

Â  Â  Â  Â  if not reranked:
Â  Â  Â  Â  Â  Â  return gerar_resposta_fallback_interativa(pergunta, model_id=model_id)

Â  Â  Â  Â  best_score = reranked[0]["score"]
Â  Â  Â  Â  # Usando os novos thresholds
Â  Â  Â  Â  pass_threshold = (best_score >= (CE_SCORE_THRESHOLD if run_ce else ANN_SCORE_THRESHOLD))

Â  Â  Â  Â  top_texts = [r["block"]["texto"] for r in reranked]
Â  Â  Â  Â  evidence_ok = _has_lexical_evidence(pergunta, top_texts)

Â  Â  Â  Â  if not pass_threshold and evidence_ok:
Â  Â  Â  Â  Â  Â  pass_threshold = True

Â  Â  Â  Â  if pass_threshold:
Â  Â  Â  Â  Â  Â  blocos_relevantes = [r["block"] for r in reranked]
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  blocos_relevantes = [r["block"] for r in (reranked or candidates[:TOP_K])]

Â  Â  Â  Â  t_rag = time.perf_counter()

Â  Â  Â  Â  # 3) Monta prompt
Â  Â  Â  Â  prompt = montar_prompt_rag(pergunta, blocos_relevantes)

Â  Â  Â  Â  payload = {
Â  Â  Â  Â  Â  Â  "model": model_id,
Â  Â  Â  Â  Â  Â  "messages": [
Â  Â  Â  Â  Â  Â  Â  Â  {"role": "system", "content": "VocÃª responde apenas com base no conteÃºdo fornecido."},
Â  Â  Â  Â  Â  Â  Â  Â  {"role": "user", "content": prompt}
Â  Â  Â  Â  Â  Â  ],
Â  Â  Â  Â  Â  Â  "max_tokens": MAX_TOKENS,
Â  Â  Â  Â  Â  Â  "temperature": TEMPERATURE,
Â  Â  Â  Â  Â  Â  "n": 1,
Â  Â  Â  Â  Â  Â  "stream": False
Â  Â  Â  Â  }

Â  Â  Â  Â  # 4) Chamada Ã  API da OpenAI
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  resp = session.post(
Â  Â  Â  Â  Â  Â  Â  Â  "https://api.openai.com/v1/chat/completions",
Â  Â  Â  Â  Â  Â  Â  Â  json=payload,
Â  Â  Â  Â  Â  Â  Â  Â  timeout=REQUEST_TIMEOUT
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  resp.raise_for_status()
Â  Â  Â  Â  Â  Â  data = resp.json()
Â  Â  Â  Â  Â  Â  resposta_final = (
Â  Â  Â  Â  Â  Â  Â  Â  data.get("choices", [{}])[0]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .get("message", {})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  .get("content", "")
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  except requests.exceptions.RequestException as e:
Â  Â  Â  Â  Â  Â  return f"âŒ Erro de conexÃ£o com a API: {e}"
Â  Â  Â  Â  except (ValueError, KeyError, IndexError):
Â  Â  Â  Â  Â  Â  return "âš ï¸ NÃ£o consegui interpretar a resposta da API."

Â  Â  Â  Â  if not resposta_final or not resposta_final.strip():
Â  Â  Â  Â  Â  Â  return "âš ï¸ A resposta da API veio vazia ou incompleta."

Â  Â  Â  Â  resposta = resposta_final.strip()
Â  Â  Â  Â  t_api = time.perf_counter()

Â  Â  Â  Â  # 5) PÃ³s-processamento
Â  Â  Â  Â  if _looks_like_noinfo(resposta) or _is_fallback_output(resposta):
Â  Â  Â  Â  Â  Â  # Se o LLM cair no fallback, chamamos a funÃ§Ã£o interativa para dar o toque amigÃ¡vel
Â  Â  Â  Â  Â  Â  return gerar_resposta_fallback_interativa(pergunta, model_id=model_id)

Â  Â  Â  Â  if blocos_relevantes:
Â  Â  Â  Â  Â  Â  bloco_link = _escolher_bloco_para_link(pergunta, resposta, blocos_relevantes)
Â  Â  Â  Â  Â  Â  if bloco_link:
Â  Â  Â  Â  Â  Â  Â  Â  doc_id = bloco_link.get("file_id")
Â  Â  Â  Â  Â  Â  Â  Â  raw_nome = bloco_link.get("pagina", "?")
Â  Â  Â  Â  Â  Â  Â  Â  doc_nome = sanitize_doc_name(raw_nome)
Â  Â  Â  Â  Â  Â  Â  Â  if doc_id:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  link = f"https://drive.google.com/file/d/{doc_id}/view?usp=sharing"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  resposta += f"\n\nğŸ“„ Documento relacionado: {doc_nome}\nğŸ”— {link}"

Â  Â  Â  Â  t_end = time.perf_counter()
Â  Â  Â  Â  print(
Â  Â  Â  Â  Â  Â  f"[DEBUG POP-BOT] RAG: {t_rag - t0:.2f}s | OpenAI: {t_api - t_rag:.2f}s | Total responder_pergunta: {t_end - t0:.2f}s"
Â  Â  Â  Â  )

Â  Â  Â  Â  return resposta

Â  Â  except Exception as e:
Â  Â  Â  Â  return f"âŒ Erro interno: {e}"


# ========================= CLI =========================
if __name__ == "__main__":
Â  Â  print("\nDigite sua pergunta (ou 'sair'):\n")
Â  Â  while True:
Â  Â  Â  Â  q = input("Pergunta: ").strip()
Â  Â  Â  Â  if q.lower() in ("sair", "exit", "quit"):
Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  print("\nResposta:\n" + "="*20)
Â  Â  Â  Â  print(responder_pergunta(q))
Â  Â  Â  Â  print("="*20 + "\n")
